{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677d8a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Malathi\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d7beb5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  News ID Category         Topic  \\\n",
      "0  N10000   sports        soccer   \n",
      "1  N10001     news  newspolitics   \n",
      "2  N10002     news        newsus   \n",
      "3  N10003     news  newspolitics   \n",
      "4  N10004     news     newsworld   \n",
      "\n",
      "                                            Headline  \\\n",
      "0  Predicting Atlanta United's lineup against Col...   \n",
      "1  Mitch McConnell: DC statehood push is 'full bo...   \n",
      "2            Home In North Highlands Damaged By Fire   \n",
      "3  Meghan McCain blames 'liberal media' and 'thir...   \n",
      "4                            Today in History: Aug 1   \n",
      "\n",
      "                                           News body  \\\n",
      "0  Only FIVE internationals allowed, count em, FI...   \n",
      "1  WASHINGTON -- Senate Majority Leader Mitch McC...   \n",
      "2  NORTH HIGHLANDS (CBS13)   Fire damaged a home ...   \n",
      "3  Meghan McCain is speaking out after a journali...   \n",
      "4  1714: George I becomes King Georg Ludwig, Elec...   \n",
      "\n",
      "                                Title entity  \\\n",
      "0  {\"Atlanta United's\": 'Atlanta United FC'}   \n",
      "1                 {'DC': 'Washington, D.C.'}   \n",
      "2                                         {}   \n",
      "3                                         {}   \n",
      "4                                         {}   \n",
      "\n",
      "                                      Entity content  \n",
      "0  {'Atlanta United FC': {'type': 'item', 'id': '...  \n",
      "1  {'Washington, D.C.': {'type': 'item', 'id': 'Q...  \n",
      "2                                                 {}  \n",
      "3                                                 {}  \n",
      "4                                                 {}  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Malathi M/OneDrive/Documents/MDTE25/guvi final project/Main project/news.tsv.zip\", sep=\"\\t\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c017ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News ID            0\n",
       "Category           0\n",
       "Topic              0\n",
       "Headline           0\n",
       "News body         58\n",
       "Title entity       0\n",
       "Entity content     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d5e948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Headline\", \"News body\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63d80bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News ID           0\n",
       "Category          0\n",
       "Topic             0\n",
       "Headline          0\n",
       "News body         0\n",
       "Title entity      0\n",
       "Entity content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "191ef4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['news id', 'category', 'topic', 'headline', 'news body', 'title entity',\n",
       "       'entity content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cb22a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Headline + News body\n",
    "df['text'] = (df['Headline'].str.strip() + ' ' + df['News body'].str.strip()).str.strip()\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Feature and target\n",
    "X = df[\"text\"]\n",
    "y = df[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54ab85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only required columns\n",
    "df = df[[\"Headline\", \"Title entity\", \"Entity content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64c9513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 50% of the data to reduce computation\n",
    "df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1bad7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_dict(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "df[\"title_entity_dict\"] = df[\"Title entity\"].apply(parse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5abe28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_ner(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", str(text))\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clean_text\"] = df[\"Headline\"].apply(clean_text_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b3235da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c6aa7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ffac8d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA READY â€” Samples: 56881\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df[\"Headline\"] = df[\"Headline\"].astype(str)\n",
    "df[\"Title entity\"] = df[\"Title entity\"].astype(str)\n",
    "\n",
    "COUNTRIES = [\"United States\", \"India\", \"Brazil\", \"China\", \"Mexico\", \"Canada\"]\n",
    "PERSON_PATTERN = r\"^[A-Z][a-z]+(\\s[A-Z][a-z]+)+$\"\n",
    "ORG_KEYWORDS = [\"Corporation\", \"Authority\", \"Committee\", \"Association\", \"University\", \"Agency\", \"Company\", \"FC\", \"Ltd\"]\n",
    "\n",
    "\n",
    "def infer_entity_type(expanded):\n",
    "    expanded = expanded.strip()\n",
    "\n",
    "    if re.match(PERSON_PATTERN, expanded):\n",
    "        return \"PERSON\"\n",
    "\n",
    "    if expanded in COUNTRIES:\n",
    "        return \"LOCATION\"\n",
    "\n",
    "    if any(k in expanded for k in ORG_KEYWORDS):\n",
    "        return \"ORG\"\n",
    "\n",
    "    return \"MISC\"\n",
    "\n",
    "def convert_to_bio(text, entity_string):\n",
    "    tokens = text.split()\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    if entity_string == \"{}\":\n",
    "        return tokens, tags\n",
    "\n",
    "    try:\n",
    "        ent_dict = ast.literal_eval(entity_string)\n",
    "    except:\n",
    "        return tokens, tags\n",
    "\n",
    "    lower_tokens = [w.lower().strip(\".,!?\") for w in tokens]\n",
    "\n",
    "    for surface, expanded in ent_dict.items():\n",
    "        clean_surface = surface.replace(\"'s\", \"\").strip()\n",
    "        stoks = clean_surface.split()\n",
    "        stoks = [w.lower().strip(\".,!?\") for w in stoks]\n",
    "        n = len(stoks)\n",
    "\n",
    "        ent_type = infer_entity_type(expanded)\n",
    "\n",
    "        # Search entity span safely\n",
    "        for i in range(len(tokens)):\n",
    "            try:\n",
    "                if lower_tokens[i:i+n] == stoks:\n",
    "                    tags[i] = f\"B-{ent_type}\"\n",
    "                    for j in range(i+1, i+n):\n",
    "                        if j < len(tags):   # SAFETY CHECK\n",
    "                            tags[j] = f\"I-{ent_type}\"\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    s, t = convert_to_bio(row[\"Headline\"], row[\"Title entity\"])\n",
    "    sentences.append(s)\n",
    "    labels.append(t)\n",
    "\n",
    "print(\"DATA READY â€” Samples:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "987f29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ner_tags\"] = df.apply(\n",
    "    lambda row: create_ner_tags(\n",
    "        row[\"tokens\"],\n",
    "        row[\"Entity content\"],\n",
    "        row[\"Title entity\"]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dbbba84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"tokens\"].apply(len) == df[\"ner_tags\"].apply(len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "896f093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 28440\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ner_dataset = Dataset.from_pandas(df[[\"tokens\", \"ner_tags\"]])\n",
    "ner_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "160b4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example BIO labels â€“ adjust if needed\n",
    "label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "96f675f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "74211dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],              # list of word tokens\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "\n",
    "    aligned_labels = []\n",
    "\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)    # padding / special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[labels[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)    # subword â†’ ignore\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "645675c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d0af5a071a45c5ae17b97c05c00544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35978bcb5f8e41ad98f5b3eb19f7fa21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Example: you already created these\n",
    "# tokens â†’ list[list[str]]\n",
    "# ner_tags â†’ list[list[str]]\n",
    "\n",
    "df_small = df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"tokens\": df_small[\"tokens\"].tolist(),\n",
    "    \"ner_tags\": df_small[\"ner_tags\"].tolist()\n",
    "})\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7287162f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "db3f2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_ner\",\n",
    "    max_steps=850,          # ðŸ”´ STOP at step 850\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6489ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:\n",
    "                true_preds.append(id2label[p_i])\n",
    "                true_labels.append(id2label[l_i])\n",
    "\n",
    "    report = classification_report(true_labels, true_preds, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fdd8669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malathi M\\AppData\\Local\\Temp\\ipykernel_26568\\3924159620.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\Malathi M\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='850' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [850/850 1:16:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=850, training_loss=0.013946388042531907, metrics={'train_runtime': 4609.9836, 'train_samples_per_second': 1.475, 'train_steps_per_second': 0.184, 'total_flos': 444224566579200.0, 'train_loss': 0.013946388042531907, 'epoch': 0.5977496483825597})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a54ccad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_ner_final\\\\tokenizer_config.json',\n",
       " './bert_ner_final\\\\special_tokens_map.json',\n",
       " './bert_ner_final\\\\vocab.txt',\n",
       " './bert_ner_final\\\\added_tokens.json',\n",
       " './bert_ner_final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bert_ner_final\")\n",
    "tokenizer.save_pretrained(\"./bert_ner_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74554cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d156338",
   "metadata": {},
   "source": [
    "Model\trouge1\trouge2\trougeL\trougeLsum\tAverage Score\n",
    "bert\t0.74819271\t0.686270584\t0.6888\t0.6884\t0.7077\n",
    "lstm_bahdanau\t0.685955\t0.300094\t0.5139\t0.2253\t0.5433\n",
    "TextRank\t0.5028\t0.3924\t0.4402\t0.4402\t0.4451\n",
    "TF-IDF\t0.4836\t0.3779\t0.4144\t0.4144\t0.4253"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
