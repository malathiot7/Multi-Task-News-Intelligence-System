{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1622733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Malathi\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0dfe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\malathi m\\anaconda3\\lib\\site-packages (2.20.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-addons\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow tensorflow-addons seqeval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbea3c0",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d6ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  News ID Category         Topic  \\\n",
      "0  N10000   sports        soccer   \n",
      "1  N10001     news  newspolitics   \n",
      "2  N10002     news        newsus   \n",
      "3  N10003     news  newspolitics   \n",
      "4  N10004     news     newsworld   \n",
      "\n",
      "                                            Headline  \\\n",
      "0  Predicting Atlanta United's lineup against Col...   \n",
      "1  Mitch McConnell: DC statehood push is 'full bo...   \n",
      "2            Home In North Highlands Damaged By Fire   \n",
      "3  Meghan McCain blames 'liberal media' and 'thir...   \n",
      "4                            Today in History: Aug 1   \n",
      "\n",
      "                                           News body  \\\n",
      "0  Only FIVE internationals allowed, count em, FI...   \n",
      "1  WASHINGTON -- Senate Majority Leader Mitch McC...   \n",
      "2  NORTH HIGHLANDS (CBS13)   Fire damaged a home ...   \n",
      "3  Meghan McCain is speaking out after a journali...   \n",
      "4  1714: George I becomes King Georg Ludwig, Elec...   \n",
      "\n",
      "                                Title entity  \\\n",
      "0  {\"Atlanta United's\": 'Atlanta United FC'}   \n",
      "1                 {'DC': 'Washington, D.C.'}   \n",
      "2                                         {}   \n",
      "3                                         {}   \n",
      "4                                         {}   \n",
      "\n",
      "                                      Entity content  \n",
      "0  {'Atlanta United FC': {'type': 'item', 'id': '...  \n",
      "1  {'Washington, D.C.': {'type': 'item', 'id': 'Q...  \n",
      "2                                                 {}  \n",
      "3                                                 {}  \n",
      "4                                                 {}  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Malathi M/OneDrive/Documents/MDTE25/guvi final project/Main project/news.tsv.zip\", sep=\"\\t\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1784b3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "sports           30557\n",
       "news             26689\n",
       "finance          10571\n",
       "lifestyle         7405\n",
       "autos             5494\n",
       "travel            5381\n",
       "foodanddrink      5286\n",
       "video             4968\n",
       "tv                3981\n",
       "health            3799\n",
       "weather           3298\n",
       "music             2547\n",
       "movies            1996\n",
       "entertainment     1487\n",
       "kids               299\n",
       "europe               2\n",
       "northamerica         1\n",
       "adexperience         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b642b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News ID            0\n",
       "Category           0\n",
       "Topic              0\n",
       "Headline           0\n",
       "News body         58\n",
       "Title entity       0\n",
       "Entity content     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ad2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values to avoid errors\n",
    "df['Headline'] = df.get('Headline', '').fillna('').astype(str)\n",
    "df['News body'] = df.get('News body', '').fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8292d72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['News ID',\n",
       " 'Category',\n",
       " 'Topic',\n",
       " 'Headline',\n",
       " 'News body',\n",
       " 'Title entity',\n",
       " 'Entity content']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a00896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News ID           0\n",
       "Category          0\n",
       "Topic             0\n",
       "Headline          0\n",
       "News body         0\n",
       "Title entity      0\n",
       "Entity content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec298e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113762, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ad2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Headline + News body\n",
    "df['text'] = (df['Headline'].str.strip() + ' ' + df['News body'].str.strip()).str.strip()\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Feature and target\n",
    "X = df[\"text\"]\n",
    "y = df[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"TEXT:\", X.iloc[i])\n",
    "    print(\"CATEGORY:\", y.iloc[i])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e242600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only required columns\n",
    "df = df[[\"Headline\", \"Title entity\", \"Entity content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468e9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 50% of the data to reduce computation\n",
    "df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab39b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Title entity</th>\n",
       "      <th>Entity content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stars who came out</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19 Ice Cream Pies You'll Want to Make All Summ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mixed-used development will bring variety to d...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul Manafort Seemed Headed to Rikers. Then th...</td>\n",
       "      <td>{'Justice Department': 'United States Departme...</td>\n",
       "      <td>{'United States Department of Justice': {'type...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sean Shelby's Shoes: What's next for Junior Do...</td>\n",
       "      <td>{'Junior Dos Santos': 'Junior dos Santos', 'UF...</td>\n",
       "      <td>{'Junior dos Santos': {'type': 'item', 'id': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0                                 Stars who came out   \n",
       "1  19 Ice Cream Pies You'll Want to Make All Summ...   \n",
       "2  Mixed-used development will bring variety to d...   \n",
       "3  Paul Manafort Seemed Headed to Rikers. Then th...   \n",
       "4  Sean Shelby's Shoes: What's next for Junior Do...   \n",
       "\n",
       "                                        Title entity  \\\n",
       "0                                                 {}   \n",
       "1                                                 {}   \n",
       "2                                                 {}   \n",
       "3  {'Justice Department': 'United States Departme...   \n",
       "4  {'Junior Dos Santos': 'Junior dos Santos', 'UF...   \n",
       "\n",
       "                                      Entity content  \n",
       "0                                                 {}  \n",
       "1                                                 {}  \n",
       "2                                                 {}  \n",
       "3  {'United States Department of Justice': {'type...  \n",
       "4  {'Junior dos Santos': {'type': 'item', 'id': '...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f4b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_dict(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "df[\"title_entity_dict\"] = df[\"Title entity\"].apply(parse_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_ner(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", str(text))\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clean_text\"] = df[\"Headline\"].apply(clean_text_ner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "552a519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "568ec6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6914fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA READY — Samples: 56881\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df[\"Headline\"] = df[\"Headline\"].astype(str)\n",
    "df[\"Title entity\"] = df[\"Title entity\"].astype(str)\n",
    "\n",
    "COUNTRIES = [\"United States\", \"India\", \"Brazil\", \"China\", \"Mexico\", \"Canada\"]\n",
    "PERSON_PATTERN = r\"^[A-Z][a-z]+(\\s[A-Z][a-z]+)+$\"\n",
    "ORG_KEYWORDS = [\"Corporation\", \"Authority\", \"Committee\", \"Association\", \"University\", \"Agency\", \"Company\", \"FC\", \"Ltd\"]\n",
    "\n",
    "\n",
    "def infer_entity_type(expanded):\n",
    "    expanded = expanded.strip()\n",
    "\n",
    "    if re.match(PERSON_PATTERN, expanded):\n",
    "        return \"PERSON\"\n",
    "\n",
    "    if expanded in COUNTRIES:\n",
    "        return \"LOCATION\"\n",
    "\n",
    "    if any(k in expanded for k in ORG_KEYWORDS):\n",
    "        return \"ORG\"\n",
    "\n",
    "    return \"MISC\"\n",
    "\n",
    "def convert_to_bio(text, entity_string):\n",
    "    tokens = text.split()\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    if entity_string == \"{}\":\n",
    "        return tokens, tags\n",
    "\n",
    "    try:\n",
    "        ent_dict = ast.literal_eval(entity_string)\n",
    "    except:\n",
    "        return tokens, tags\n",
    "\n",
    "    lower_tokens = [w.lower().strip(\".,!?\") for w in tokens]\n",
    "\n",
    "    for surface, expanded in ent_dict.items():\n",
    "        clean_surface = surface.replace(\"'s\", \"\").strip()\n",
    "        stoks = clean_surface.split()\n",
    "        stoks = [w.lower().strip(\".,!?\") for w in stoks]\n",
    "        n = len(stoks)\n",
    "\n",
    "        ent_type = infer_entity_type(expanded)\n",
    "\n",
    "        # Search entity span safely\n",
    "        for i in range(len(tokens)):\n",
    "            try:\n",
    "                if lower_tokens[i:i+n] == stoks:\n",
    "                    tags[i] = f\"B-{ent_type}\"\n",
    "                    for j in range(i+1, i+n):\n",
    "                        if j < len(tags):   # SAFETY CHECK\n",
    "                            tags[j] = f\"I-{ent_type}\"\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    s, t = convert_to_bio(row[\"Headline\"], row[\"Title entity\"])\n",
    "    sentences.append(s)\n",
    "    labels.append(t)\n",
    "\n",
    "print(\"DATA READY — Samples:\", len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras tensorflow\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "083e5f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE: 72461\n",
      "NUM TAGS: 8\n"
     ]
    }
   ],
   "source": [
    "# ENCODING & VOCAB GENERATION\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "word_set = set(w for s in sentences for w in s)\n",
    "tag_set = set(t for seq in labels for t in seq)\n",
    "\n",
    "#word2idx = {w: i+2 for i, w in enumerate(sorted(word_set))\n",
    "word2idx = {w: i for i, w in enumerate(word2idx.keys())}\n",
    "\n",
    "word2idx[\"\"] = 0\n",
    "word2idx[\"\"] = 1\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(sorted(tag_set))}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "MAX_LEN = 60\n",
    "\n",
    "X = [[word2idx.get(w, 1) for w in seq] for seq in sentences]\n",
    "X = pad_sequences(X, maxlen=MAX_LEN, padding=\"post\")\n",
    "\n",
    "y = [[tag2idx[t] for t in seq] for seq in labels]\n",
    "y = pad_sequences(y, maxlen=MAX_LEN, padding=\"post\")\n",
    "y_cat = to_categorical(y, num_classes=len(tag2idx))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "NUM_TAGS = len(tag2idx)\n",
    "\n",
    "print(\"VOCAB SIZE:\", VOCAB_SIZE)\n",
    "print(\"NUM TAGS:\", NUM_TAGS)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5937b6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72459, 72461)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(word2idx.values()), len(word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24e67375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Word2vec\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "embedding_w2v = np.zeros((VOCAB_SIZE, 100))\n",
    "for w, i in word2idx.items():\n",
    "    embedding_w2v[i] = w2v_model.wv[w] if w in w2v_model.wv else np.random.normal(0,0.6,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d30a1f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings found: 400000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GloVe\n",
    "\n",
    "GLOVE_PATH = \"C:/Users/Malathi M/OneDrive/Documents/MDTE25/guvi final project/Main project/NER/glove.6B.100d.txt\"\n",
    "import numpy as np\n",
    "\n",
    "glove_index = {}\n",
    "with open(GLOVE_PATH, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        glove_index[word] = vector\n",
    "\n",
    "print(\"Total embeddings found:\", len(glove_index))\n",
    "\n",
    "# Matrix\n",
    "\n",
    "embedding_glove = np.zeros((VOCAB_SIZE, 100))\n",
    "\n",
    "for w, i in word2idx.items():\n",
    "    embedding_glove[i] = glove_index.get(\n",
    "        w, np.random.normal(scale=0.6, size=(100,))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2a748",
   "metadata": {},
   "source": [
    "Rule-based / Heuristic Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1dfe05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea2fbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERNS = {\n",
    "    \"PERSON\": re.compile(r\"\\b([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)\\b\"),\n",
    "    \n",
    "    \"DATE\": re.compile(\n",
    "        r\"\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\"      # 12/05/2023\n",
    "        r\"\\d{4}|\"                                # 2023\n",
    "        r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s\\d{1,2},?\\s\\d{4})\\b\",\n",
    "        re.IGNORECASE\n",
    "    ),\n",
    "\n",
    "    \"ORG\": re.compile(\n",
    "        r\"\\b([A-Z][a-zA-Z& ]+(?:Inc|Ltd|LLC|Corp|University|Institute))\\b\"\n",
    "    ),\n",
    "\n",
    "    \"LOC\": re.compile(\n",
    "        r\"\\b(New York|London|Paris|India|USA|Germany|China)\\b\"\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fcce6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_ner(text):\n",
    "    entities = []\n",
    "\n",
    "    for label, pattern in PATTERNS.items():\n",
    "        for match in pattern.finditer(text):\n",
    "            entities.append({\n",
    "                \"text\": match.group(),\n",
    "                \"label\": label,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d24c49d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Barack Obama', 'label': 'PERSON', 'start': 0, 'end': 12}\n",
      "{'text': 'New York', 'label': 'PERSON', 'start': 21, 'end': 29}\n",
      "{'text': 'July', 'label': 'PERSON', 'start': 33, 'end': 37}\n",
      "{'text': 'Harvard University', 'label': 'PERSON', 'start': 59, 'end': 77}\n",
      "{'text': 'July 4, 2012', 'label': 'DATE', 'start': 33, 'end': 45}\n",
      "{'text': 'Harvard University', 'label': 'ORG', 'start': 59, 'end': 77}\n",
      "{'text': 'New York', 'label': 'LOC', 'start': 21, 'end': 29}\n"
     ]
    }
   ],
   "source": [
    "text = \"Barack Obama visited New York on July 4, 2012 and spoke at Harvard University.\"\n",
    "\n",
    "entities = rule_based_ner(text)\n",
    "\n",
    "for e in entities:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "240dc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_to_bio(tokens, entities):\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    for ent in entities:\n",
    "        ent_tokens = ent[\"text\"].split()\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i:i+len(ent_tokens)] == ent_tokens:\n",
    "                tags[i] = f\"B-{ent['label']}\"\n",
    "                for j in range(1, len(ent_tokens)):\n",
    "                    tags[i+j] = f\"I-{ent['label']}\"\n",
    "\n",
    "    return list(zip(tokens, tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "114986ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Barack', 'B-PERSON'),\n",
       " ('Obama', 'I-PERSON'),\n",
       " ('visited', 'O'),\n",
       " ('New', 'B-LOC'),\n",
       " ('York', 'I-LOC'),\n",
       " ('on', 'O'),\n",
       " ('July', 'B-DATE'),\n",
       " ('4,', 'I-DATE'),\n",
       " ('2012', 'I-DATE'),\n",
       " ('and', 'O'),\n",
       " ('spoke', 'O'),\n",
       " ('at', 'O'),\n",
       " ('Harvard', 'O'),\n",
       " ('University.', 'O')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "bio_tags = ner_to_bio(tokens, entities)\n",
    "bio_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3471e",
   "metadata": {},
   "source": [
    "BILSTM,BILSTM-CRF  DL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip uninstall -y tensorflow keras\n",
    "# !pip install tensorflow==2.11 keras==2.11 tensorflow-addons==0.20.0\n",
    "# !pip install keras-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69440e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install tensorflow-addons==0.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd8d154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "\n",
    "# ============================================\n",
    "# Regular BiLSTM Model\n",
    "# ============================================\n",
    "\n",
    "def build_bilstm(embedding_matrix):\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    emb = Embedding(\n",
    "        VOCAB_SIZE, 100,\n",
    "        weights=[embedding_matrix],\n",
    "        mask_zero=True,\n",
    "        trainable=False\n",
    "    )(inp)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(emb)\n",
    "    out = TimeDistributed(Dense(NUM_TAGS, activation=\"softmax\"))(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# BiLSTM + CRF-like loss\n",
    "# (NO tensorflow_addons needed)\n",
    "# ============================================\n",
    "\n",
    "def build_bilstm_crf(embedding_matrix):\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    emb = Embedding(\n",
    "        VOCAB_SIZE, 100,\n",
    "        weights=[embedding_matrix],\n",
    "        mask_zero=True,\n",
    "        trainable=False\n",
    "    )(inp)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(emb)\n",
    "    logits = TimeDistributed(Dense(NUM_TAGS))(x)\n",
    "\n",
    "    # Learnable CRF transition matrix\n",
    "    transitions = tf.Variable(\n",
    "        tf.random.uniform(shape=(NUM_TAGS, NUM_TAGS)),\n",
    "        name=\"transition_matrix\"\n",
    "    )\n",
    "\n",
    "    def crf_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true → one-hot target\n",
    "        y_pred → logits\n",
    "        \"\"\"\n",
    "        y_true_idx = tf.argmax(y_true, axis=-1)  # (batch, seq)\n",
    "\n",
    "        # emission log probabilities\n",
    "        log_softmax = tf.nn.log_softmax(y_pred, axis=-1)\n",
    "\n",
    "        # likelihood of correct token prediction\n",
    "        token_ll = tf.reduce_sum(\n",
    "            tf.reduce_sum(\n",
    "                tf.one_hot(y_true_idx, NUM_TAGS) * log_softmax,\n",
    "                axis=-1\n",
    "            ),\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        # transition score\n",
    "        seq_score = 0.0\n",
    "\n",
    "        for t in range(MAX_LEN - 1):\n",
    "            curr = y_true_idx[:, t]\n",
    "            nxt = y_true_idx[:, t + 1]\n",
    "\n",
    "            seq_score += tf.gather_nd(\n",
    "                transitions,\n",
    "                tf.stack([curr, nxt], axis=1)\n",
    "            )\n",
    "\n",
    "        loss = -(token_ll + seq_score)\n",
    "\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    model = Model(inp, logits)\n",
    "    model.compile(optimizer=\"adam\", loss=crf_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86632436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MODEL-1: BiLSTM + Word2Vec\n",
      "Epoch 1/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 127ms/step - accuracy: 0.9083 - loss: 0.3610 - val_accuracy: 0.9099 - val_loss: 0.3237\n",
      "Epoch 2/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 127ms/step - accuracy: 0.9115 - loss: 0.3108 - val_accuracy: 0.9113 - val_loss: 0.3069\n",
      "Epoch 3/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 120ms/step - accuracy: 0.9134 - loss: 0.2960 - val_accuracy: 0.9132 - val_loss: 0.2989\n",
      "Epoch 4/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 119ms/step - accuracy: 0.9149 - loss: 0.2855 - val_accuracy: 0.9139 - val_loss: 0.2923\n",
      "Epoch 5/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 130ms/step - accuracy: 0.9165 - loss: 0.2766 - val_accuracy: 0.9142 - val_loss: 0.2917\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "models_dict = {}\n",
    "\n",
    "print(\"\\nTraining MODEL-1: BiLSTM + Word2Vec\")\n",
    "model_1 = build_bilstm(embedding_w2v)\n",
    "model_1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n",
    "models_dict[\"1\"] = model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "738124ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MODEL-2: BiLSTM + GloVe\n",
      "Epoch 1/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 124ms/step - accuracy: 0.9086 - loss: 0.3570 - val_accuracy: 0.9148 - val_loss: 0.2987\n",
      "Epoch 2/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 134ms/step - accuracy: 0.9230 - loss: 0.2604 - val_accuracy: 0.9261 - val_loss: 0.2491\n",
      "Epoch 3/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 139ms/step - accuracy: 0.9335 - loss: 0.2159 - val_accuracy: 0.9319 - val_loss: 0.2307\n",
      "Epoch 4/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 124ms/step - accuracy: 0.9416 - loss: 0.1833 - val_accuracy: 0.9330 - val_loss: 0.2223\n",
      "Epoch 5/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 129ms/step - accuracy: 0.9493 - loss: 0.1556 - val_accuracy: 0.9349 - val_loss: 0.2232\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining MODEL-2: BiLSTM + GloVe\")\n",
    "model_2 = build_bilstm(embedding_glove)\n",
    "model_2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n",
    "models_dict[\"2\"] = model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6263dbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MODEL-3: BiLSTM-CRF + Word2Vec\n",
      "Epoch 1/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 247ms/step - loss: 18.7810 - val_loss: -13.2679\n",
      "Epoch 2/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 243ms/step - loss: -24.3470 - val_loss: -30.8737\n",
      "Epoch 3/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 245ms/step - loss: -33.5180 - val_loss: -35.1961\n",
      "Epoch 4/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 244ms/step - loss: -36.2035 - val_loss: -36.7597\n",
      "Epoch 5/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 241ms/step - loss: -37.2827 - val_loss: -37.4397\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining MODEL-3: BiLSTM-CRF + Word2Vec\")\n",
    "model_3 = build_bilstm_crf(embedding_w2v)\n",
    "model_3.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n",
    "models_dict[\"3\"] = model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5afd680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MODEL-4: BiLSTM-CRF + GloVe\n",
      "Epoch 1/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 247ms/step - loss: 54.4582 - val_loss: 22.0937\n",
      "Epoch 2/5\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 252ms/step - loss: 10.7813 - val_loss: 4.1989\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining MODEL-4: BiLSTM-CRF + GloVe\")\n",
    "model_4 = build_bilstm_crf(embedding_glove)\n",
    "model_4.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[early_stop])\n",
    "models_dict[\"4\"] = model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ad569c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_prob = model.predict(X_test)\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "    recall    = recall_score(y_test, y_pred, average=\"binary\")\n",
    "    f1        = f1_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1428b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seqeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a2fc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_ner_model(model, X_val, y_val, idx2tag):\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # If CRF → output already decoded\n",
    "    if len(y_pred.shape) == 2:\n",
    "        y_pred_ids = y_pred\n",
    "    else:\n",
    "        y_pred_ids = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "\n",
    "    for true_seq, pred_seq in zip(y_val, y_pred_ids):\n",
    "        true_tags.append([idx2tag[t] for t in true_seq if t != 0])\n",
    "        pred_tags.append([idx2tag[p] for p in pred_seq if p != 0])\n",
    "\n",
    "    return (\n",
    "        precision_score(true_tags, pred_tags),\n",
    "        recall_score(true_tags, pred_tags),\n",
    "        f1_score(true_tags, pred_tags)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d537a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_ner_model(model, X_val, y_val, idx2tag):\n",
    "    # Model predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Convert predictions to tag indices\n",
    "    if len(y_pred.shape) == 2:  # CRF decoded output\n",
    "        y_pred_ids = y_pred\n",
    "    else:\n",
    "        y_pred_ids = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Convert one-hot labels to indices\n",
    "    if len(y_val.shape) == 3:  # one-hot\n",
    "        y_true_ids = np.argmax(y_val, axis=-1)\n",
    "    else:\n",
    "        y_true_ids = y_val\n",
    "\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "\n",
    "    for true_seq, pred_seq in zip(y_true_ids, y_pred_ids):\n",
    "        t_tags = []\n",
    "        p_tags = []\n",
    "\n",
    "        for t, p in zip(true_seq, pred_seq):\n",
    "            if t == 0:  # skip PAD\n",
    "                continue\n",
    "            t_tags.append(idx2tag[int(t)])\n",
    "            p_tags.append(idx2tag[int(p)])\n",
    "\n",
    "        true_tags.append(t_tags)\n",
    "        pred_tags.append(p_tags)\n",
    "\n",
    "    return (\n",
    "        precision_score(true_tags, pred_tags),\n",
    "        recall_score(true_tags, pred_tags),\n",
    "        f1_score(true_tags, pred_tags)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed392220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 122ms/step\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 121ms/step\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 120ms/step\n",
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 117ms/step\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "model_info = {\n",
    "    \"1\": (\"BiLSTM\", \"Word2Vec\"),\n",
    "    \"2\": (\"BiLSTM\", \"GloVe\"),\n",
    "    \"3\": (\"BiLSTM-CRF\", \"Word2Vec\"),\n",
    "    \"4\": (\"BiLSTM-CRF\", \"GloVe\")\n",
    "}\n",
    "\n",
    "for key, model in models_dict.items():\n",
    "    precision, recall, f1 = evaluate_ner_model(\n",
    "        model, X_val, y_val, idx2tag\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_info[key][0],\n",
    "        \"Embedding\": model_info[key][1],\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1_Score\": round(f1, 4)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "891d134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.5760</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.6173</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>0.5148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM-CRF</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.5233</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM-CRF</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.5867</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>0.2483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model Embedding  Precision  Recall  F1_Score\n",
       "0      BiLSTM  Word2Vec     0.5760  0.1071    0.1806\n",
       "1      BiLSTM     GloVe     0.6173  0.4415    0.5148\n",
       "2  BiLSTM-CRF  Word2Vec     0.5233  0.1684    0.2548\n",
       "3  BiLSTM-CRF     GloVe     0.5867  0.1574    0.2483"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59528b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model        BiLSTM\n",
       "Embedding     GloVe\n",
       "Precision    0.6173\n",
       "Recall       0.4415\n",
       "F1_Score     0.5148\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_row = results_df.loc[results_df[\"F1_Score\"].idxmax()]\n",
    "best_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5cf5d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_key = results_df[\"F1_Score\"].idxmax() + 1\n",
    "best_model = models_dict[str(best_model_key)]\n",
    "best_model.save(\"best_ner_model.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
