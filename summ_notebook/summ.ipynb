{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "582d1e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Malathi\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Malathi\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec, KeyedVectors \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "import networkx as nx\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84575231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43217ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas numpy scikit-learn nltk rouge-score torch transformers sentencepiece tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ca69509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  News ID Category         Topic  \\\n",
      "0  N10000   sports        soccer   \n",
      "1  N10001     news  newspolitics   \n",
      "2  N10002     news        newsus   \n",
      "3  N10003     news  newspolitics   \n",
      "4  N10004     news     newsworld   \n",
      "\n",
      "                                            Headline  \\\n",
      "0  Predicting Atlanta United's lineup against Col...   \n",
      "1  Mitch McConnell: DC statehood push is 'full bo...   \n",
      "2            Home In North Highlands Damaged By Fire   \n",
      "3  Meghan McCain blames 'liberal media' and 'thir...   \n",
      "4                            Today in History: Aug 1   \n",
      "\n",
      "                                           News body  \\\n",
      "0  Only FIVE internationals allowed, count em, FI...   \n",
      "1  WASHINGTON -- Senate Majority Leader Mitch McC...   \n",
      "2  NORTH HIGHLANDS (CBS13)   Fire damaged a home ...   \n",
      "3  Meghan McCain is speaking out after a journali...   \n",
      "4  1714: George I becomes King Georg Ludwig, Elec...   \n",
      "\n",
      "                                Title entity  \\\n",
      "0  {\"Atlanta United's\": 'Atlanta United FC'}   \n",
      "1                 {'DC': 'Washington, D.C.'}   \n",
      "2                                         {}   \n",
      "3                                         {}   \n",
      "4                                         {}   \n",
      "\n",
      "                                      Entity content  \n",
      "0  {'Atlanta United FC': {'type': 'item', 'id': '...  \n",
      "1  {'Washington, D.C.': {'type': 'item', 'id': 'Q...  \n",
      "2                                                 {}  \n",
      "3                                                 {}  \n",
      "4                                                 {}  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Malathi M/OneDrive/Documents/MDTE25/guvi final project/Main project/news.tsv.zip\", sep=\"\\t\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39b94536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News ID</th>\n",
       "      <th>label</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Headline</th>\n",
       "      <th>News body</th>\n",
       "      <th>Title entity</th>\n",
       "      <th>Entity content</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N95333</td>\n",
       "      <td>news</td>\n",
       "      <td>newsus</td>\n",
       "      <td>This dog's smile will melt your heart</td>\n",
       "      <td>Mocca lives in Yokohama, Japan, and is a Shiba...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>This dog's smile will melt your heart Mocca li...</td>\n",
       "      <td>This dog's smile will melt your heart</td>\n",
       "      <td>This dog's smile will melt your heart Mocca li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N41910</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>shop-all</td>\n",
       "      <td>The Most Popular Walmart Item in Every State</td>\n",
       "      <td>What are the most oft-ordered Walmart products...</td>\n",
       "      <td>{'Walmart': 'Walmart'}</td>\n",
       "      <td>{'Walmart': {'type': 'item', 'id': 'Q18615334'...</td>\n",
       "      <td>The Most Popular Walmart Item in Every State W...</td>\n",
       "      <td>The Most Popular Walmart Item in Every State</td>\n",
       "      <td>The Most Popular Walmart Item in Every State W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N88506</td>\n",
       "      <td>finance</td>\n",
       "      <td>finance-real-estate</td>\n",
       "      <td>Photos: Look Glenn Close's 'Beanfield' estate ...</td>\n",
       "      <td>Emmy winning actress Glen Close listed her Bed...</td>\n",
       "      <td>{\"Glenn Close's\": 'Glenn Close', 'Bedford': 'B...</td>\n",
       "      <td>{'Glenn Close': {'type': 'item', 'id': 'Q37231...</td>\n",
       "      <td>Photos: Look Glenn Close's 'Beanfield' estate ...</td>\n",
       "      <td>Photos: Look Glenn Close's 'Beanfield' estate ...</td>\n",
       "      <td>Photos: Look Glenn Close's 'Beanfield' estate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N114168</td>\n",
       "      <td>news</td>\n",
       "      <td>newscrime</td>\n",
       "      <td>Hillsborough Sheriff's Office sweep results in...</td>\n",
       "      <td>TAMPA   More than 80 people have been arrested...</td>\n",
       "      <td>{'human trafficking': 'Human trafficking'}</td>\n",
       "      <td>{'Human trafficking': {'type': 'item', 'id': '...</td>\n",
       "      <td>Hillsborough Sheriff's Office sweep results in...</td>\n",
       "      <td>Hillsborough Sheriff's Office sweep results in...</td>\n",
       "      <td>Hillsborough Sheriff's Office sweep results in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N35279</td>\n",
       "      <td>video</td>\n",
       "      <td>peopleandplaces</td>\n",
       "      <td>Family of missing Connecticut mom blast 'Gone ...</td>\n",
       "      <td>Family members and friends of Jennifer Dulos s...</td>\n",
       "      <td>{'Connecticut': 'Connecticut'}</td>\n",
       "      <td>{'Connecticut': {'type': 'item', 'id': 'Q58425...</td>\n",
       "      <td>Family of missing Connecticut mom blast 'Gone ...</td>\n",
       "      <td>Family of missing Connecticut mom blast 'Gone ...</td>\n",
       "      <td>Family of missing Connecticut mom blast 'Gone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   News ID      label                Topic  \\\n",
       "0   N95333       news               newsus   \n",
       "1   N41910  lifestyle             shop-all   \n",
       "2   N88506    finance  finance-real-estate   \n",
       "3  N114168       news            newscrime   \n",
       "4   N35279      video      peopleandplaces   \n",
       "\n",
       "                                            Headline  \\\n",
       "0              This dog's smile will melt your heart   \n",
       "1       The Most Popular Walmart Item in Every State   \n",
       "2  Photos: Look Glenn Close's 'Beanfield' estate ...   \n",
       "3  Hillsborough Sheriff's Office sweep results in...   \n",
       "4  Family of missing Connecticut mom blast 'Gone ...   \n",
       "\n",
       "                                           News body  \\\n",
       "0  Mocca lives in Yokohama, Japan, and is a Shiba...   \n",
       "1  What are the most oft-ordered Walmart products...   \n",
       "2  Emmy winning actress Glen Close listed her Bed...   \n",
       "3  TAMPA   More than 80 people have been arrested...   \n",
       "4  Family members and friends of Jennifer Dulos s...   \n",
       "\n",
       "                                        Title entity  \\\n",
       "0                                                 {}   \n",
       "1                             {'Walmart': 'Walmart'}   \n",
       "2  {\"Glenn Close's\": 'Glenn Close', 'Bedford': 'B...   \n",
       "3         {'human trafficking': 'Human trafficking'}   \n",
       "4                     {'Connecticut': 'Connecticut'}   \n",
       "\n",
       "                                      Entity content  \\\n",
       "0                                                 {}   \n",
       "1  {'Walmart': {'type': 'item', 'id': 'Q18615334'...   \n",
       "2  {'Glenn Close': {'type': 'item', 'id': 'Q37231...   \n",
       "3  {'Human trafficking': {'type': 'item', 'id': '...   \n",
       "4  {'Connecticut': {'type': 'item', 'id': 'Q58425...   \n",
       "\n",
       "                                                text  \\\n",
       "0  This dog's smile will melt your heart Mocca li...   \n",
       "1  The Most Popular Walmart Item in Every State W...   \n",
       "2  Photos: Look Glenn Close's 'Beanfield' estate ...   \n",
       "3  Hillsborough Sheriff's Office sweep results in...   \n",
       "4  Family of missing Connecticut mom blast 'Gone ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0              This dog's smile will melt your heart   \n",
       "1       The Most Popular Walmart Item in Every State   \n",
       "2  Photos: Look Glenn Close's 'Beanfield' estate ...   \n",
       "3  Hillsborough Sheriff's Office sweep results in...   \n",
       "4  Family of missing Connecticut mom blast 'Gone ...   \n",
       "\n",
       "                                             article  \n",
       "0  This dog's smile will melt your heart Mocca li...  \n",
       "1  The Most Popular Walmart Item in Every State W...  \n",
       "2  Photos: Look Glenn Close's 'Beanfield' estate ...  \n",
       "3  Hillsborough Sheriff's Office sweep results in...  \n",
       "4  Family of missing Connecticut mom blast 'Gone ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=[\"Headline\", \"News body\", \"Category\"])\n",
    "\n",
    "df[\"text\"] = df[\"Headline\"].astype(str) + \" \" + df[\"News body\"].astype(str)\n",
    "df[\"summary\"] = df[\"Headline\"].astype(str)\n",
    "\n",
    "df = df.rename(columns={\"Category\": \"label\"})\n",
    "\n",
    "# Use only 50%\n",
    "df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df[\"article\"] = df[\"text\"]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4b52f",
   "metadata": {},
   "source": [
    " Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81ad25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    text = text.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    text = re.sub(r\"[^a-zA-Z0-9 ]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_article\"] = df[\"article\"].apply(clean_text)\n",
    "df[\"clean_summary\"] = df[\"summary\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f49a8",
   "metadata": {},
   "source": [
    "TF-IDF Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee126938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    scores = tfidf_matrix.mean(axis=1).A.flatten()\n",
    "    ranked_idx = np.argsort(scores)[::-1][:top_n]\n",
    "\n",
    "    selected = sorted(ranked_idx)\n",
    "    return \" \".join([sentences[i] for i in selected])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c04e63",
   "metadata": {},
   "source": [
    "TextRank Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1a2daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    selected = sorted([idx for idx, _ in ranked[:top_n]])\n",
    "\n",
    "    return \" \".join([sentences[i] for i in selected])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57249e2",
   "metadata": {},
   "source": [
    "Seq2Seq (LSTM) – TINY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "953ffd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=3000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"article\"])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df[\"article\"])\n",
    "y = tokenizer.texts_to_sequences(df[\"summary\"])\n",
    "\n",
    "max_len = 80\n",
    "X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "y = pad_sequences(y, maxlen=max_len, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6aba83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">96,000</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">96,000</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │                   │            │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">99,000</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │     \u001b[38;5;34m96,000\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │     \u001b[38;5;34m96,000\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m),      │      \u001b[38;5;34m8,320\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m8,320\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │                   │            │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m3000\u001b[0m)  │     \u001b[38;5;34m99,000\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">307,640</span> (1.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m307,640\u001b[0m (1.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">307,640</span> (1.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m307,640\u001b[0m (1.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 3000\n",
    "\n",
    "enc_inputs = Input(shape=(max_len,))\n",
    "enc_emb = Embedding(vocab_size, 32)(enc_inputs)\n",
    "enc_lstm = LSTM(32, return_state=True)\n",
    "_, h, c = enc_lstm(enc_emb)\n",
    "\n",
    "dec_inputs = Input(shape=(max_len,))\n",
    "dec_emb = Embedding(vocab_size, 32)(dec_inputs)\n",
    "dec_lstm = LSTM(32, return_sequences=True)\n",
    "dec_out = dec_lstm(dec_emb, initial_state=[h, c])\n",
    "dec_dense = Dense(vocab_size, activation=\"softmax\")\n",
    "outputs = dec_dense(dec_out)\n",
    "\n",
    "seq2seq_model = Model([enc_inputs, dec_inputs], outputs)\n",
    "seq2seq_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "seq2seq_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b4c08e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m3554/3554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 156ms/step - loss: 0.6756\n",
      "Epoch 2/3\n",
      "\u001b[1m3554/3554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 158ms/step - loss: 0.2508\n",
      "Epoch 3/3\n",
      "\u001b[1m3554/3554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 155ms/step - loss: 0.0914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a1ff9e5810>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_model.fit(\n",
    "    [X, y],\n",
    "    y,\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e006a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_summarize(text, max_len=20):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    seq = pad_sequences(seq, maxlen=80, padding=\"post\")\n",
    "    preds = seq2seq_model.predict([seq, seq], verbose=0)\n",
    "    pred_ids = np.argmax(preds[0], axis=1)\n",
    "    return \" \".join(tokenizer.index_word.get(i, \"\") for i in pred_ids if i != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30c358ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "def evaluate(model_fn, df, n=50):\n",
    "    r1, r2, rL = [], [], []\n",
    "\n",
    "    for i in range(min(n, len(df))):\n",
    "        ref = df.iloc[i][\"summary\"]\n",
    "        pred = model_fn(df.iloc[i][\"article\"])\n",
    "        score = scorer.score(ref, pred)\n",
    "        r1.append(score[\"rouge1\"].fmeasure)\n",
    "        r2.append(score[\"rouge2\"].fmeasure)\n",
    "        rL.append(score[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return np.mean(r1), np.mean(r2), np.mean(rL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a051de7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.159553</td>\n",
       "      <td>0.148539</td>\n",
       "      <td>0.159553</td>\n",
       "      <td>0.155882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>0.159553</td>\n",
       "      <td>0.148539</td>\n",
       "      <td>0.159553</td>\n",
       "      <td>0.155882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seq2Seq</td>\n",
       "      <td>0.124501</td>\n",
       "      <td>0.057310</td>\n",
       "      <td>0.112941</td>\n",
       "      <td>0.098250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model   ROUGE-1   ROUGE-2   ROUGE-L   Average\n",
       "0    TF-IDF  0.159553  0.148539  0.159553  0.155882\n",
       "1  TextRank  0.159553  0.148539  0.159553  0.155882\n",
       "2   Seq2Seq  0.124501  0.057310  0.112941  0.098250"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "results.append((\"TF-IDF\", *evaluate(tfidf_summarize, df)))\n",
    "results.append((\"TextRank\", *evaluate(textrank_summarize, df)))\n",
    "results.append((\"Seq2Seq\", *evaluate(seq2seq_summarize, df)))\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Model\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n",
    ")\n",
    "\n",
    "results_df[\"Average\"] = results_df[[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]].mean(axis=1)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29d864",
   "metadata": {},
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f2619dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: TF-IDF\n"
     ]
    }
   ],
   "source": [
    "best_model = results_df.sort_values(\"Average\", ascending=False).iloc[0][\"Model\"]\n",
    "\n",
    "config = {\n",
    "    \"best_model\": best_model,\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "joblib.dump(config, \"best_from_scratch_summarizer.pkl\")\n",
    "results_df.to_csv(\"from_scratch_summarization_results.csv\", index=False)\n",
    "\n",
    "print(\"Best model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "def evaluate_model(summarizer_fn, df, samples=50):\n",
    "    r1, r2, rL = [], [], []\n",
    "\n",
    "    for i in range(min(samples, len(df))):\n",
    "        text = df.iloc[i][\"article\"]\n",
    "        ref = df.iloc[i][\"summary\"]\n",
    "        pred = summarizer_fn(text)\n",
    "\n",
    "        scores = scorer.score(ref, pred)\n",
    "        r1.append(scores[\"rouge1\"].fmeasure)\n",
    "        r2.append(scores[\"rouge2\"].fmeasure)\n",
    "        rL.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": np.mean(r1),\n",
    "        \"rouge2\": np.mean(r2),\n",
    "        \"rougeL\": np.mean(rL)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_scores = evaluate_model(tfidf_summarize, df)\n",
    "textrank_scores = evaluate_model(textrank_summarize, df)\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    {\"Model\": \"TF-IDF\", **tfidf_scores},\n",
    "    {\"Model\": \"TextRank\", **textrank_scores}\n",
    "])\n",
    "\n",
    "results[\"Average\"] = results[[\"rouge1\", \"rouge2\", \"rougeL\"]].mean(axis=1)\n",
    "results.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aade123",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"rouge_eval_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a948de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results.sort_values(\"Average\", ascending=False).iloc[0][\"Model\"]\n",
    "\n",
    "best_model = {\n",
    "    \"name\": best_model_name,\n",
    "    \"top_n\": 3\n",
    "}\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model, \"best_summarizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d4c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Headline', 'News body'])\n",
    "df['Headline'] = df['Headline'].astype(str)\n",
    "df['News body'] = df['News body'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9cc710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News ID           0\n",
       "Category          0\n",
       "Topic             0\n",
       "Headline          0\n",
       "News body         0\n",
       "Title entity      0\n",
       "Entity content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2577de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine headline and body\n",
    "df[\"text\"] = df[\"Headline\"].fillna(\"\") + \" \" + df[\"News body\"].fillna(\"\")\n",
    "\n",
    "# Rename label column\n",
    "df = df.rename(columns={\"Category\": \"label\"})\n",
    "\n",
    "# Drop rows missing essential fields\n",
    "df = df.dropna(subset=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119853a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 50% of the data to reduce computation\n",
    "df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d79e4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is required for evaluation\n",
    "df[\"full_text\"] = df[\"text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7483c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "df[\"article\"] = df[\"full_text\"]\n",
    "\n",
    "# Ground-truth summary\n",
    "df[\"summary\"] = df[\"Headline\"]\n",
    "\n",
    "df = df.dropna(subset=[\"article\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805aa7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def clean_text(text):\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    # Replace punctuation with space\n",
    "    text = text.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', ' ', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6fca96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    this dog s smile will melt your heart mocca li...\n",
       "1    the most popular walmart item in every state w...\n",
       "2    photos look glenn close s beanfield estate in ...\n",
       "3    hillsborough sheriff s office sweep results in...\n",
       "4    family of missing connecticut mom blast gone g...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "df[\"clean_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d1f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_article\"] = df[\"article\"].apply(clean_text)\n",
    "df[\"clean_summary\"] = df[\"summary\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56568de5",
   "metadata": {},
   "source": [
    "sentence tokanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e57adda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this dog s smile will melt your heart mocca l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[the most popular walmart item in every state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[photos look glenn close s beanfield estate in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[hillsborough sheriff s office sweep results i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[family of missing connecticut mom blast gone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  [this dog s smile will melt your heart mocca l...\n",
       "1  [the most popular walmart item in every state ...\n",
       "2  [photos look glenn close s beanfield estate in...\n",
       "3  [hillsborough sheriff s office sweep results i...\n",
       "4  [family of missing connecticut mom blast gone ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_split(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "df['sentences'] = df['clean_article'].apply(sentence_split)\n",
    "df[['sentences']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00895335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e6fe844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c485ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEXT RANK SUMMARIZER\n",
    "\n",
    "def textrank_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    summary = \" \".join([s for _, s in ranked[:top_n]])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65d2741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF SENTENCE SCORING\n",
    "\n",
    "def tfidf_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    scores = tfidf_matrix.mean(axis=1).A.flatten()\n",
    "\n",
    "    ranked_idx = np.argsort(scores)[::-1]\n",
    "    selected = [sentences[i] for i in ranked_idx[:top_n]]\n",
    "\n",
    "    return \" \".join(selected)\n",
    "\n",
    "\n",
    "# REFERENCE SUMMARY (WEAK BASELINE)\n",
    "\n",
    "def reference_summary(text):\n",
    "    sents = sent_tokenize(clean_text(text))\n",
    "    return \" \".join(sents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d77a0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROUGE EVALUATION\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def evaluate_model(summarizer_fn, df, samples=50):\n",
    "\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    for i in range(min(samples, len(df))):\n",
    "\n",
    "        text = df.iloc[i][\"full_text\"]\n",
    "        ref = reference_summary(text)\n",
    "        pred = summarizer_fn(text)\n",
    "\n",
    "        scores = scorer.score(ref, pred)\n",
    "\n",
    "        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": np.mean(rouge1_scores),\n",
    "        \"rouge2\": np.mean(rouge2_scores),\n",
    "        \"rougeL\": np.mean(rougeL_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af33084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TextRank\n",
      "Evaluating TF-IDF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RUN EVALUATION\n",
    "\n",
    "print(\"Evaluating TextRank\")\n",
    "textrank_scores = evaluate_model(textrank_summarize, df)\n",
    "\n",
    "print(\"Evaluating TF-IDF\")\n",
    "tfidf_scores = evaluate_model(tfidf_summarize, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b85dbc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Completed.\n",
      "Saved to: rouge_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "# SAVE RESULTS TO CSV\n",
    "\n",
    "OUTPUT_CSV = \"rouge_eval_results.csv\"  # change if needed\n",
    "\n",
    "new_results = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"TextRank\",\n",
    "        \"rouge1\": textrank_scores[\"rouge1\"],\n",
    "        \"rouge2\": textrank_scores[\"rouge2\"],\n",
    "        \"rougeL\": textrank_scores[\"rougeL\"],\n",
    "        \"rougeLsum\": textrank_scores[\"rougeL\"],\n",
    "        \"Average Score\": np.mean([\n",
    "            textrank_scores[\"rouge1\"],\n",
    "            textrank_scores[\"rouge2\"],\n",
    "            textrank_scores[\"rougeL\"],\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"TF-IDF\",\n",
    "        \"rouge1\": tfidf_scores[\"rouge1\"],\n",
    "        \"rouge2\": tfidf_scores[\"rouge2\"],\n",
    "        \"rougeL\": tfidf_scores[\"rougeL\"],\n",
    "        \"rougeLsum\": tfidf_scores[\"rougeL\"],\n",
    "        \"Average Score\": np.mean([\n",
    "            tfidf_scores[\"rouge1\"],\n",
    "            tfidf_scores[\"rouge2\"],\n",
    "            tfidf_scores[\"rougeL\"],\n",
    "        ])\n",
    "    }\n",
    "])\n",
    "new_results = new_results.round(4)\n",
    "\n",
    "# If CSV exists → append, else create new\n",
    "try:\n",
    "    old_df = pd.read_csv(OUTPUT_CSV)\n",
    "    final_df = pd.concat([old_df, new_results], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    final_df = new_results\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"\\nEvaluation Completed.\\nSaved to:\", OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d012dddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  rouge1  rouge2  rougeL  rougeLsum  Average Score\n",
       "0  TextRank     1.0     1.0     1.0        1.0            1.0\n",
       "1    TF-IDF     1.0     1.0     1.0        1.0            1.0\n",
       "2  TextRank     1.0     1.0     1.0        1.0            1.0\n",
       "3    TF-IDF     1.0     1.0     1.0        1.0            1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv(\"C:/Users/Malathi M/OneDrive/Documents/MDTE25/guvi final project/Main project/summarization/rouge_eval_results.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d952aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this dog s smile will melt your heart mocca lives in yokohama japan and is a shiba inu with a unique smile when greeting someone this dog can t hide its happiness eyes partly closed smile wide ears going horizontal plus a tail wagging like a helicopter say it all\n"
     ]
    }
   ],
   "source": [
    "sample_text = df['full_text'][0]\n",
    "sam = textrank_summarize(sample_text)\n",
    "print(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ summarizer_functions.py saved\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: CREATE SUMMARIZER FUNCTIONS FILE\n",
    "\n",
    "summarizer_code = \"\"\"\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle empty or NaN-like values\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    import re\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    # Keep only letters, numbers, basic punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \" \", text)\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tfidf_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    scores = tfidf_matrix.sum(axis=1).A.flatten()\n",
    "\n",
    "    ranked_idx = sorted(scores.argsort()[::-1][:top_n])\n",
    "    return \" \".join([sentences[i] for i in ranked_idx])\n",
    "\n",
    "def summarize(text, model_name=\"TF-IDF\", top_n=3):\n",
    "    if model_name == \"TF-IDF\":\n",
    "        return tfidf_summarize(text, top_n)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown summarization model\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../Summarization/summarizer_functions.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summarizer_code)\n",
    "\n",
    "print(\"✅ summarizer_functions.py saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e95944f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best summarizer saved\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_model = {\n",
    "    \"name\": \"TF-IDF\",\n",
    "    \"top_n\": 3\n",
    "}\n",
    "\n",
    "joblib.dump(best_model, \"../Summarization/best_summarizer_TF-IDF.pkl\")\n",
    "\n",
    "print(\"✅ Best summarizer saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53881859",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m best_model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Summarization/best_summarizer_TF-IDF.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews body\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize(\n\u001b[0;32m      8\u001b[0m     text,\n\u001b[0;32m      9\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mbest_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m     top_n\u001b[38;5;241m=\u001b[39mbest_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_n\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUMMARY:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "File \u001b[1;32mc:\\Users\\Malathi M\\OneDrive\\Documents\\MDTE25\\guvi final project\\Main project\\summarization\\summarizer_functions.py:54\u001b[0m, in \u001b[0;36msummarize\u001b[1;34m(text, model_name, top_n)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msummarize\u001b[39m(text, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tfidf_summarize(text, top_n)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown summarization model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Malathi M\\OneDrive\\Documents\\MDTE25\\guvi final project\\Main project\\summarization\\summarizer_functions.py:40\u001b[0m, in \u001b[0;36mtfidf_summarize\u001b[1;34m(text, top_n)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtfidf_summarize\u001b[39m(text, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     cleaned \u001b[38;5;241m=\u001b[39m clean_text(text)\n\u001b[0;32m     41\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(cleaned)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m top_n:\n",
      "File \u001b[1;32mc:\\Users\\Malathi M\\OneDrive\\Documents\\MDTE25\\guvi final project\\Main project\\summarization\\summarizer_functions.py:11\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(text):\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from summarizer_functions import summarize\n",
    "\n",
    "best_model = joblib.load(\"../Summarization/best_summarizer_TF-IDF.pkl\")\n",
    "\n",
    "text = df.loc[0, \"News body\"]\n",
    "\n",
    "summary = summarize(\n",
    "    text,\n",
    "    model_name=best_model[\"name\"],\n",
    "    top_n=best_model[\"top_n\"]\n",
    ")\n",
    "\n",
    "print(\"SUMMARY:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = {\n",
    "    \"name\": best_model_name,\n",
    "    \"top_n\": 3\n",
    "}\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model, f\"../Summarization/best_summarizer_{best_model_name}.pkl\")\n",
    "\n",
    "summarizer_code = \"\"\"\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CLEANING FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"http\\\\S+|www\\\\S+|https\\\\S+\", \" \", text)\n",
    "    text = re.sub(\"[\" \n",
    "                  u\"\\\\U0001F600-\\\\U0001F64F\"\n",
    "                  u\"\\\\U0001F300-\\\\U0001F5FF\"\n",
    "                  u\"\\\\U0001F680-\\\\U0001F6FF\"\n",
    "                  u\"\\\\U0001F1E0-\\\\U0001F1FF\"\n",
    "                  \"]+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\\\s.,!?]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "    \n",
    "# ---------------------------------------------------------\n",
    "# TEXT RANK SUMMARY\n",
    "# ---------------------------------------------------------\n",
    "def textrank_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    return \" \".join([s for _, s in ranked[:top_n]])\n",
    "    # ---------------------------------------------------------\n",
    "# TF-IDF SCORING SUMMARY\n",
    "# ---------------------------------------------------------\n",
    "def tfidf_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    scores = tfidf_matrix.mean(axis=1).A.flatten()\n",
    "    ranked_idx = scores.argsort()[::-1]\n",
    "    selected = [sentences[i] for i in ranked_idx[:top_n]]\n",
    "    return \" \".join(selected)\n",
    "    \n",
    "# ---------------------------------------------------------\n",
    "# UNIFIED SUMMARIZER FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def summarize(text, model_name, top_n=3):\n",
    "    if model_name == \"TextRank\":\n",
    "        return textrank_summarize(text, top_n)\n",
    "    elif model_name == \"TF-IDF\":\n",
    "        return tfidf_summarize(text, top_n)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown summarization model: \" + model_name)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"summarizer_functions.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summarizer_code)\n",
    "\n",
    "print(\"Saved summarizer functions → ../Summarization/summarizer_functions.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8a3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
