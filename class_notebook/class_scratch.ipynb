{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80296633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Malathi\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Malathi\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d595757a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News ID</th>\n",
       "      <th>category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Headline</th>\n",
       "      <th>News body</th>\n",
       "      <th>Title entity</th>\n",
       "      <th>Entity content</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N95333</td>\n",
       "      <td>news</td>\n",
       "      <td>newsus</td>\n",
       "      <td>This dog's smile will melt your heart</td>\n",
       "      <td>Mocca lives in Yokohama, Japan, and is a Shiba...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>This dog's smile will melt your heart Mocca li...</td>\n",
       "      <td>This dog's smile will melt your heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N41910</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>shop-all</td>\n",
       "      <td>The Most Popular Walmart Item in Every State</td>\n",
       "      <td>What are the most oft-ordered Walmart products...</td>\n",
       "      <td>{'Walmart': 'Walmart'}</td>\n",
       "      <td>{'Walmart': {'type': 'item', 'id': 'Q18615334'...</td>\n",
       "      <td>The Most Popular Walmart Item in Every State W...</td>\n",
       "      <td>The Most Popular Walmart Item in Every State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N88506</td>\n",
       "      <td>finance</td>\n",
       "      <td>finance-real-estate</td>\n",
       "      <td>Photos: Look Glenn Close's 'Beanfield' estate ...</td>\n",
       "      <td>Emmy winning actress Glen Close listed her Bed...</td>\n",
       "      <td>{\"Glenn Close's\": 'Glenn Close', 'Bedford': 'B...</td>\n",
       "      <td>{'Glenn Close': {'type': 'item', 'id': 'Q37231...</td>\n",
       "      <td>Photos: Look Glenn Close's 'Beanfield' estate ...</td>\n",
       "      <td>Photos: Look Glenn Close's 'Beanfield' estate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N114168</td>\n",
       "      <td>news</td>\n",
       "      <td>newscrime</td>\n",
       "      <td>Hillsborough Sheriff's Office sweep results in...</td>\n",
       "      <td>TAMPA   More than 80 people have been arrested...</td>\n",
       "      <td>{'human trafficking': 'Human trafficking'}</td>\n",
       "      <td>{'Human trafficking': {'type': 'item', 'id': '...</td>\n",
       "      <td>Hillsborough Sheriff's Office sweep results in...</td>\n",
       "      <td>Hillsborough Sheriff's Office sweep results in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N35279</td>\n",
       "      <td>video</td>\n",
       "      <td>peopleandplaces</td>\n",
       "      <td>Family of missing Connecticut mom blast 'Gone ...</td>\n",
       "      <td>Family members and friends of Jennifer Dulos s...</td>\n",
       "      <td>{'Connecticut': 'Connecticut'}</td>\n",
       "      <td>{'Connecticut': {'type': 'item', 'id': 'Q58425...</td>\n",
       "      <td>Family of missing Connecticut mom blast 'Gone ...</td>\n",
       "      <td>Family of missing Connecticut mom blast 'Gone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   News ID   category                Topic  \\\n",
       "0   N95333       news               newsus   \n",
       "1   N41910  lifestyle             shop-all   \n",
       "2   N88506    finance  finance-real-estate   \n",
       "3  N114168       news            newscrime   \n",
       "4   N35279      video      peopleandplaces   \n",
       "\n",
       "                                            Headline  \\\n",
       "0              This dog's smile will melt your heart   \n",
       "1       The Most Popular Walmart Item in Every State   \n",
       "2  Photos: Look Glenn Close's 'Beanfield' estate ...   \n",
       "3  Hillsborough Sheriff's Office sweep results in...   \n",
       "4  Family of missing Connecticut mom blast 'Gone ...   \n",
       "\n",
       "                                           News body  \\\n",
       "0  Mocca lives in Yokohama, Japan, and is a Shiba...   \n",
       "1  What are the most oft-ordered Walmart products...   \n",
       "2  Emmy winning actress Glen Close listed her Bed...   \n",
       "3  TAMPA   More than 80 people have been arrested...   \n",
       "4  Family members and friends of Jennifer Dulos s...   \n",
       "\n",
       "                                        Title entity  \\\n",
       "0                                                 {}   \n",
       "1                             {'Walmart': 'Walmart'}   \n",
       "2  {\"Glenn Close's\": 'Glenn Close', 'Bedford': 'B...   \n",
       "3         {'human trafficking': 'Human trafficking'}   \n",
       "4                     {'Connecticut': 'Connecticut'}   \n",
       "\n",
       "                                      Entity content  \\\n",
       "0                                                 {}   \n",
       "1  {'Walmart': {'type': 'item', 'id': 'Q18615334'...   \n",
       "2  {'Glenn Close': {'type': 'item', 'id': 'Q37231...   \n",
       "3  {'Human trafficking': {'type': 'item', 'id': '...   \n",
       "4  {'Connecticut': {'type': 'item', 'id': 'Q58425...   \n",
       "\n",
       "                                                text  \\\n",
       "0  This dog's smile will melt your heart Mocca li...   \n",
       "1  The Most Popular Walmart Item in Every State W...   \n",
       "2  Photos: Look Glenn Close's 'Beanfield' estate ...   \n",
       "3  Hillsborough Sheriff's Office sweep results in...   \n",
       "4  Family of missing Connecticut mom blast 'Gone ...   \n",
       "\n",
       "                                             summary  \n",
       "0              This dog's smile will melt your heart  \n",
       "1       The Most Popular Walmart Item in Every State  \n",
       "2  Photos: Look Glenn Close's 'Beanfield' estate ...  \n",
       "3  Hillsborough Sheriff's Office sweep results in...  \n",
       "4  Family of missing Connecticut mom blast 'Gone ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"C:/Users/Malathi M/OneDrive/Documents/MDTE25/guvi final project/Main project/news.tsv.zip\",sep=\"\\t\")\n",
    "\n",
    "df = df.dropna(subset=[\"Headline\", \"News body\", \"Category\"])\n",
    "\n",
    "df[\"text\"] = df[\"Headline\"].astype(str) + \" \" + df[\"News body\"].astype(str)\n",
    "df[\"summary\"] = df[\"Headline\"].astype(str)\n",
    "df = df.rename(columns={\"Category\": \"category\"})\n",
    "\n",
    "# Use only 50% (low compute)\n",
    "df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ea5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    text = text.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa8ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [w for w in word_tokenize(text) if w not in stop_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ac9725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finance</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category  label\n",
       "0       news     11\n",
       "1  lifestyle      8\n",
       "2    finance      4\n",
       "3       news     11\n",
       "4      video     16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"category\"])\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "df[[\"category\", \"label\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7237b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "13    15163\n",
       "11    13392\n",
       "4      5259\n",
       "8      3729\n",
       "1      2779\n",
       "14     2674\n",
       "5      2645\n",
       "16     2469\n",
       "15     2005\n",
       "6      1880\n",
       "17     1621\n",
       "10     1316\n",
       "9      1011\n",
       "2       758\n",
       "7       147\n",
       "3         2\n",
       "12        1\n",
       "0         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad6e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df[\"label\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "\n",
    "df = df[df[\"label\"].isin(valid_classes)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f71f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df[\"clean_text\"],\n",
    "    df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c20c1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(max_features=5000)\n",
    "X_train_bow = bow.fit_transform(X_train_text)\n",
    "X_test_bow = bow.transform(X_test_text)\n",
    "\n",
    "pickle.dump(bow, open(\"bow_vectorizer.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf869d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "pickle.dump(tfidf, open(\"tfidf_vectorizer.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6afc1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "sentences = df[\"tokens\"].tolist()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=2,\n",
    "    workers=2,\n",
    "    sg=0\n",
    ")\n",
    "\n",
    "pickle.dump(w2v_model, open(\"word2vec_model.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0933448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(tokens, model):\n",
    "    tokens = [w for w in tokens if w in model.wv]\n",
    "    if not tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[tokens], axis=0)\n",
    "\n",
    "X_w2v = np.array([document_vector(t, w2v_model) for t in df[\"tokens\"]])\n",
    "\n",
    "X_train_w2v, X_test_w2v, _, _ = train_test_split(\n",
    "    X_w2v, df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4cd40c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malathi M\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LogReg + BoW\": LogisticRegression(max_iter=500),\n",
    "    \"LogReg + TFIDF\": LogisticRegression(max_iter=500),\n",
    "    \"LogReg + W2V\": LogisticRegression(max_iter=500)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "models[\"LogReg + BoW\"].fit(X_train_bow, y_train)\n",
    "models[\"LogReg + TFIDF\"].fit(X_train_tfidf, y_train)\n",
    "models[\"LogReg + W2V\"].fit(X_train_w2v, y_train)\n",
    "\n",
    "results.append([\"LogReg + BoW\", accuracy_score(y_test, models[\"LogReg + BoW\"].predict(X_test_bow))])\n",
    "results.append([\"LogReg + TFIDF\", accuracy_score(y_test, models[\"LogReg + TFIDF\"].predict(X_test_tfidf))])\n",
    "results.append([\"LogReg + W2V\", accuracy_score(y_test, models[\"LogReg + W2V\"].predict(X_test_w2v))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b965f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
    "X_pad = pad_sequences(X_seq, maxlen=100)\n",
    "\n",
    "pickle.dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f77dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad, X_test_pad, y_train_dl, y_test_dl = train_test_split(\n",
    "    X_pad, df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ff9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = w2v_model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f839e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m2274/2274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 74ms/step - accuracy: 0.6260 - loss: 1.2225 - val_accuracy: 0.6867 - val_loss: 0.9468\n",
      "Epoch 2/3\n",
      "\u001b[1m2274/2274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 70ms/step - accuracy: 0.6875 - loss: 0.9828 - val_accuracy: 0.7007 - val_loss: 0.9155\n",
      "Epoch 3/3\n",
      "\u001b[1m2274/2274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 70ms/step - accuracy: 0.7131 - loss: 0.9078 - val_accuracy: 0.7099 - val_loss: 0.8913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e0fcb2dbd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bilstm = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(le.classes_), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_bilstm.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "es = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "model_bilstm.fit(\n",
    "    X_train_pad, y_train_dl,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    callbacks=[es]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3050a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m356/356\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_dl = np.argmax(model_bilstm.predict(X_test_pad), axis=1)\n",
    "bilstm_acc = accuracy_score(y_test_dl, y_pred_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "747044b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Best ML\n",
    "joblib.dump(models[\"LogReg + TFIDF\"], \"best_ml_model.pkl\")\n",
    "\n",
    "# Best DL\n",
    "model_bilstm.save(\"best_dl_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "158f0011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg + BoW</td>\n",
       "      <td>0.725418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg + TFIDF</td>\n",
       "      <td>0.767810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogReg + W2V</td>\n",
       "      <td>0.730695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM + Word2Vec</td>\n",
       "      <td>0.701407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Accuracy\n",
       "0       LogReg + BoW  0.725418\n",
       "1     LogReg + TFIDF  0.767810\n",
       "2       LogReg + W2V  0.730695\n",
       "3  BiLSTM + Word2Vec  0.701407"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    results + [[\"BiLSTM + Word2Vec\", bilstm_acc]],\n",
    "    columns=[\"Model\", \"Accuracy\"]\n",
    ")\n",
    "\n",
    "comparison_df.to_csv(\"classification_model_comparison.csv\", index=False)\n",
    "comparison_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
